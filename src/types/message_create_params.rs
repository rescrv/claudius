use serde::{Deserialize, Serialize};

use crate::types::{
    MessageParam, Metadata, Model, SystemPrompt, TextBlock, ThinkingConfig, ToolChoice,
    ToolUnionParam,
};

/// Parameters for creating messages.
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct MessageCreateParams {
    /// The maximum number of tokens to generate before stopping.
    ///
    /// Note that our models may stop _before_ reaching this maximum. This parameter
    /// only specifies the absolute maximum number of tokens to generate.
    ///
    /// Different models have different maximum values for this parameter. See
    /// [models](https://docs.anthropic.com/en/docs/models-overview) for details.
    pub max_tokens: u32,

    /// Input messages.
    ///
    /// Our models are trained to operate on alternating `user` and `assistant`
    /// conversational turns. When creating a new `Message`, you specify the prior
    /// conversational turns with the `messages` parameter, and the model then generates
    /// the next `Message` in the conversation. Consecutive `user` or `assistant` turns
    /// in your request will be combined into a single turn.
    pub messages: Vec<MessageParam>,

    /// The model that will complete your prompt.
    ///
    /// See [models](https://docs.anthropic.com/en/docs/models-overview) for additional
    /// details and options.
    pub model: Model,

    /// An object describing metadata about the request.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub metadata: Option<Metadata>,

    /// Custom text sequences that will cause the model to stop generating.
    ///
    /// Our models will normally stop when they have naturally completed their turn,
    /// which will result in a response `stop_reason` of `"end_turn"`.
    ///
    /// If you want the model to stop generating when it encounters custom strings of
    /// text, you can use the `stop_sequences` parameter. If the model encounters one of
    /// the custom sequences, the response `stop_reason` value will be `"stop_sequence"`
    /// and the response `stop_sequence` value will contain the matched stop sequence.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stop_sequences: Option<Vec<String>>,

    /// System prompt.
    ///
    /// A system prompt is a way of providing context and instructions to Claude, such
    /// as specifying a particular goal or role. See the
    /// [guide to system prompts](https://docs.anthropic.com/en/docs/system-prompts).
    #[serde(skip_serializing_if = "Option::is_none")]
    pub system: Option<SystemPrompt>,

    /// Amount of randomness injected into the response.
    ///
    /// Defaults to `1.0`. Ranges from `0.0` to `1.0`. Use `temperature` closer to `0.0`
    /// for analytical / multiple choice, and closer to `1.0` for creative and
    /// generative tasks.
    ///
    /// Note that even with `temperature` of `0.0`, the results will not be fully
    /// deterministic.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f32>,

    /// Configuration for enabling Claude's extended thinking.
    ///
    /// When enabled, responses include `thinking` content blocks showing Claude's
    /// thinking process before the final answer. Requires a minimum budget of 1,024
    /// tokens and counts towards your `max_tokens` limit.
    ///
    /// See
    /// [extended thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking)
    /// for details.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub thinking: Option<ThinkingConfig>,

    /// How the model should use the provided tools.
    ///
    /// The model can use a specific tool, any available tool, decide by itself, or not
    /// use tools at all.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_choice: Option<ToolChoice>,

    /// Definitions of tools that the model may use.
    ///
    /// If you include `tools` in your API request, the model may return `tool_use`
    /// content blocks that represent the model's use of those tools. You can then run
    /// those tools using the tool input generated by the model and then optionally
    /// return results back to the model using `tool_result` content blocks.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tools: Option<Vec<ToolUnionParam>>,

    /// Only sample from the top K options for each subsequent token.
    ///
    /// Used to remove "long tail" low probability responses.
    /// [Learn more technical details here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).
    ///
    /// Recommended for advanced use cases only. You usually only need to use
    /// `temperature`.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_k: Option<u32>,

    /// Use nucleus sampling.
    ///
    /// In nucleus sampling, we compute the cumulative distribution over all the options
    /// for each subsequent token in decreasing probability order and cut it off once it
    /// reaches a particular probability specified by `top_p`. You should either alter
    /// `temperature` or `top_p`, but not both.
    ///
    /// Recommended for advanced use cases only. You usually only need to use
    /// `temperature`.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f32>,

    /// Whether to incrementally stream the response using server-sent events.
    ///
    /// See [streaming](https://docs.anthropic.com/en/api/messages-streaming) for
    /// details.
    pub stream: bool,
}

impl MessageCreateParams {
    /// Create a new message creation parameters with streaming disabled.
    pub fn new(max_tokens: u32, messages: Vec<MessageParam>, model: Model) -> Self {
        Self {
            max_tokens,
            messages,
            model,
            metadata: None,
            stop_sequences: None,
            system: None,
            temperature: None,
            thinking: None,
            tool_choice: None,
            tools: None,
            top_k: None,
            top_p: None,
            stream: false,
        }
    }

    /// Create new streaming message creation parameters.
    pub fn new_streaming(max_tokens: u32, messages: Vec<MessageParam>, model: Model) -> Self {
        Self {
            max_tokens,
            messages,
            model,
            metadata: None,
            stop_sequences: None,
            system: None,
            temperature: None,
            thinking: None,
            tool_choice: None,
            tools: None,
            top_k: None,
            top_p: None,
            stream: true,
        }
    }

    /// Add metadata to the parameters.
    pub fn with_metadata(mut self, metadata: Metadata) -> Self {
        self.metadata = Some(metadata);
        self
    }

    /// Add stop sequences to the parameters.
    pub fn with_stop_sequences(mut self, stop_sequences: Vec<String>) -> Self {
        self.stop_sequences = Some(stop_sequences);
        self
    }

    /// Add a system prompt as a string.
    pub fn with_system_string(mut self, system: String) -> Self {
        self.system = Some(SystemPrompt::from_string(system));
        self
    }

    /// Add a system prompt as text blocks.
    pub fn with_system_blocks(mut self, blocks: Vec<TextBlock>) -> Self {
        self.system = Some(SystemPrompt::from_blocks(blocks));
        self
    }

    /// Add a system prompt.
    pub fn with_system(mut self, system: SystemPrompt) -> Self {
        self.system = Some(system);
        self
    }

    /// Add temperature to the parameters.
    pub fn with_temperature(mut self, temperature: f32) -> Self {
        self.temperature = Some(temperature);
        self
    }

    /// Add thinking configuration to the parameters.
    pub fn with_thinking(mut self, thinking: ThinkingConfig) -> Self {
        self.thinking = Some(thinking);
        self
    }

    /// Add tool choice to the parameters.
    pub fn with_tool_choice(mut self, tool_choice: ToolChoice) -> Self {
        self.tool_choice = Some(tool_choice);
        self
    }

    /// Add tools to the parameters.
    pub fn with_tools(mut self, tools: Vec<ToolUnionParam>) -> Self {
        self.tools = Some(tools);
        self
    }

    /// Add top_k to the parameters.
    pub fn with_top_k(mut self, top_k: u32) -> Self {
        self.top_k = Some(top_k);
        self
    }

    /// Add top_p to the parameters.
    pub fn with_top_p(mut self, top_p: f32) -> Self {
        self.top_p = Some(top_p);
        self
    }

    /// Sets the streaming option.
    pub fn with_stream(mut self, stream: bool) -> Self {
        self.stream = stream;
        self
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::types::{KnownModel, MessageRole};
    use serde_json::{json, to_value};

    #[test]
    fn test_message_create_params_non_streaming() {
        let message = MessageParam::new_with_string("Hello, Claude".to_string(), MessageRole::User);

        let params = MessageCreateParams::new(
            1000,
            vec![message],
            Model::Known(KnownModel::Claude37Sonnet20250219),
        )
        .with_system_string("You are a helpful assistant.".to_string())
        .with_temperature(0.125);

        let json = to_value(&params).unwrap();
        assert!(!params.stream);

        assert_eq!(
            json,
            json!({
                "max_tokens": 1000,
                "messages": [
                    {
                        "role": "user",
                        "content": "Hello, Claude"
                    }
                ],
                "model": "claude-3-7-sonnet-20250219",
                "system": "You are a helpful assistant.",
                "temperature": 0.125,
                "stream": false
            })
        );
    }

    #[test]
    fn test_message_create_params_streaming() {
        let message = MessageParam::new_with_string("Hello, Claude".to_string(), MessageRole::User);

        let params = MessageCreateParams::new_streaming(
            1000,
            vec![message],
            Model::Known(KnownModel::Claude37Sonnet20250219),
        );

        let json = to_value(&params).unwrap();
        assert!(params.stream);

        assert_eq!(
            json,
            json!({
                "max_tokens": 1000,
                "messages": [
                    {
                        "role": "user",
                        "content": "Hello, Claude"
                    }
                ],
                "model": "claude-3-7-sonnet-20250219",
                "stream": true
            })
        );
    }

    #[test]
    fn test_message_create_params_with_stream() {
        let message = MessageParam::new_with_string("Hello, Claude".to_string(), MessageRole::User);

        let params = MessageCreateParams::new(
            1000,
            vec![message],
            Model::Known(KnownModel::Claude37Sonnet20250219),
        )
        .with_stream(true);

        let json = to_value(&params).unwrap();
        assert!(params.stream);

        assert_eq!(
            json,
            json!({
                "max_tokens": 1000,
                "messages": [
                    {
                        "role": "user",
                        "content": "Hello, Claude"
                    }
                ],
                "model": "claude-3-7-sonnet-20250219",
                "stream": true
            })
        );
    }
}
